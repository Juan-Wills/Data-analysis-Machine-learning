{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee41732",
   "metadata": {},
   "source": [
    "# Data Loading & Writing Practice Exercises\n",
    "\n",
    "Welcome to your comprehensive data loading and writing practice session! These exercises will challenge your pandas skills across different data sources and formats. Each exercise focuses on real-world scenarios you'll encounter in data analytics.\n",
    "\n",
    "## Instructions\n",
    "- Complete each exercise independently\n",
    "- Focus on proper data loading, cleaning, and writing techniques\n",
    "- Pay attention to data types, missing values, and performance considerations\n",
    "- Document your approach and any challenges you encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7706857",
   "metadata": {},
   "source": [
    "## Exercise 1: Complex CSV Analysis - World Bank Data\n",
    "**Data Source**: World Bank Open Data (CSV format)\n",
    "**URL**: https://datacatalog.worldbank.org/search/dataset/0037712\n",
    "\n",
    "**Challenge**: \n",
    "Load the World Bank's GDP data CSV file which contains:\n",
    "- Multiple header rows\n",
    "- Country codes and names\n",
    "- Time series data from 1960-2023\n",
    "- Missing values represented as \"..\"\n",
    "\n",
    "**Tasks**:\n",
    "1. Load the data properly handling the multi-header structure\n",
    "2. Clean missing values and convert to appropriate data types\n",
    "3. Reshape from wide to long format for time series analysis\n",
    "4. Filter for G7 countries only\n",
    "5. Export results to both Excel (.xlsx) and Parquet formats\n",
    "6. Create a summary statistics file in JSON format\n",
    "\n",
    "**Learning Focus**: Complex CSV parsing, data reshaping, multiple output formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cd13d",
   "metadata": {},
   "source": [
    "## Exercise 2: RESTful API Data Collection - OpenWeatherMap\n",
    "**Data Source**: OpenWeatherMap API (JSON format)\n",
    "**URL**: https://openweathermap.org/api\n",
    "\n",
    "**Challenge**: \n",
    "Work with live weather data from multiple cities using API calls:\n",
    "- Nested JSON structures\n",
    "- Rate limiting considerations\n",
    "- Real-time data with timestamps\n",
    "\n",
    "**Tasks**:\n",
    "1. Set up API authentication (free tier available)\n",
    "2. Fetch current weather data for 20 major world cities\n",
    "3. Handle nested JSON structure (weather conditions, coordinates, etc.)\n",
    "4. Combine all city data into a single DataFrame\n",
    "5. Add calculated fields (feels_like difference, wind speed categories)\n",
    "6. Store raw JSON responses in a compressed format (.gz)\n",
    "7. Export processed data to SQLite database\n",
    "8. Create a backup in pickle format\n",
    "\n",
    "**Learning Focus**: API interaction, JSON normalization, database writing, compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e362e",
   "metadata": {},
   "source": [
    "## Exercise 3: Web Scraping Challenge - Wikipedia Tables\n",
    "**Data Source**: Wikipedia (HTML tables)\n",
    "**URL**: https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\n",
    "\n",
    "**Challenge**: \n",
    "Extract and clean economic data from Wikipedia's complex HTML tables:\n",
    "- Multiple tables on same page\n",
    "- Merged cells and footnotes\n",
    "- Mixed data types and currencies\n",
    "\n",
    "**Tasks**:\n",
    "1. Scrape GDP data tables from Wikipedia\n",
    "2. Handle merged header cells and footnote references\n",
    "3. Clean currency symbols and convert to numeric values\n",
    "4. Deal with missing data and \"N/A\" entries\n",
    "5. Combine data from multiple years if available\n",
    "6. Validate data consistency across sources\n",
    "7. Export cleaned data to CSV with proper encoding (UTF-8)\n",
    "8. Create a metadata file documenting your cleaning process\n",
    "\n",
    "**Learning Focus**: HTML parsing, text cleaning, data validation, encoding handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719bde5f",
   "metadata": {},
   "source": [
    "## Exercise 4: Multi-Sheet Excel Analysis - Financial Reports\n",
    "**Data Source**: SEC EDGAR Database (Excel/XBRL format)\n",
    "**URL**: https://www.sec.gov/edgar/searchedgar/companysearch.html\n",
    "\n",
    "**Challenge**: \n",
    "Work with complex financial Excel files containing:\n",
    "- Multiple worksheets with different structures\n",
    "- Merged cells and complex formatting\n",
    "- Financial formulas and calculated fields\n",
    "\n",
    "**Tasks**:\n",
    "1. Download annual reports (10-K) in Excel format from a Fortune 500 company\n",
    "2. Load multiple sheets into separate DataFrames\n",
    "3. Handle merged cells in headers and totals\n",
    "4. Extract and clean financial statements (Balance Sheet, Income Statement, Cash Flow)\n",
    "5. Create relationships between sheets using common identifiers\n",
    "6. Calculate financial ratios across multiple periods\n",
    "7. Export consolidated data to HDF5 format for efficient storage\n",
    "8. Create separate CSV files for each financial statement\n",
    "\n",
    "**Learning Focus**: Multi-sheet Excel handling, financial data processing, HDF5 format, data relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489e857",
   "metadata": {},
   "source": [
    "## Exercise 5: Database Integration - PostgreSQL Public Datasets\n",
    "**Data Source**: PostgreSQL sample databases\n",
    "**URL**: https://www.postgresqltutorial.com/postgresql-getting-started/postgresql-sample-database/\n",
    "\n",
    "**Challenge**: \n",
    "Work with relational database containing:\n",
    "- Multiple related tables\n",
    "- Foreign key relationships\n",
    "- Large datasets requiring chunked processing\n",
    "\n",
    "**Tasks**:\n",
    "1. Set up connection to the DVD Rental sample database (or similar)\n",
    "2. Explore database schema and relationships\n",
    "3. Write complex JOIN queries to combine customer, rental, and inventory data\n",
    "4. Handle large result sets using chunked reading\n",
    "5. Perform aggregations at the database level vs. pandas level\n",
    "6. Create materialized views for common queries\n",
    "7. Export query results to multiple formats (CSV, JSON, Parquet)\n",
    "8. Implement incremental data loading for new records\n",
    "\n",
    "**Learning Focus**: SQL integration, database optimization, chunked processing, incremental loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177823e",
   "metadata": {},
   "source": [
    "## Exercise 6: XML Data Processing - RSS Feeds & SOAP APIs\n",
    "**Data Source**: Multiple RSS feeds and XML APIs\n",
    "**URL**: http://rss.cnn.com/rss/edition.rss (and others)\n",
    "\n",
    "**Challenge**: \n",
    "Parse complex XML structures with:\n",
    "- Nested elements and attributes\n",
    "- Namespaces and CDATA sections\n",
    "- Large XML files requiring streaming\n",
    "\n",
    "**Tasks**:\n",
    "1. Collect RSS feeds from 5 different news sources\n",
    "2. Parse XML structure handling namespaces properly\n",
    "3. Extract article metadata (title, date, author, categories)\n",
    "4. Handle CDATA sections and HTML content within XML\n",
    "5. Combine feeds into unified news dataset\n",
    "6. Deal with different XML schemas across sources\n",
    "7. Implement XML schema validation\n",
    "8. Export to both normalized JSON and flat CSV formats\n",
    "9. Create an XML output with your processed data\n",
    "\n",
    "**Learning Focus**: XML parsing, namespace handling, schema validation, streaming processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd3fa5",
   "metadata": {},
   "source": [
    "## Exercise 7: High-Performance Computing - NASA Climate Data (HDF5)\n",
    "**Data Source**: NASA Goddard Earth Sciences Data\n",
    "**URL**: https://disc.gsfc.nasa.gov/\n",
    "\n",
    "**Challenge**: \n",
    "Work with scientific HDF5 files containing:\n",
    "- Multi-dimensional arrays\n",
    "- Hierarchical data structures\n",
    "- Metadata and attributes\n",
    "- Very large file sizes (GB range)\n",
    "\n",
    "**Tasks**:\n",
    "1. Download climate/weather HDF5 files from NASA\n",
    "2. Explore hierarchical structure and metadata\n",
    "3. Extract specific datasets from the HDF5 groups\n",
    "4. Handle multi-dimensional arrays (time, latitude, longitude, altitude)\n",
    "5. Perform memory-efficient operations on large datasets\n",
    "6. Subset data by geographic regions and time periods\n",
    "7. Convert selected data to pandas-friendly formats\n",
    "8. Create optimized HDF5 output with custom compression\n",
    "9. Export time series data to multiple CSV files by region\n",
    "\n",
    "**Learning Focus**: HDF5 format, scientific data, memory management, hierarchical structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3ff76",
   "metadata": {},
   "source": [
    "## Exercise 8: Multi-Source Integration - E-commerce Analytics\n",
    "**Data Sources**: Mixed formats (CSV, JSON, Excel, Database)\n",
    "**Scenario**: Artificial e-commerce business data\n",
    "\n",
    "**Challenge**: \n",
    "Integrate data from multiple business systems:\n",
    "- Customer data (CSV) with encoding issues\n",
    "- Order transactions (JSON) with nested product details\n",
    "- Inventory (Excel) with multiple sheets and formulas\n",
    "- User activity logs (SQLite database)\n",
    "\n",
    "**Tasks**:\n",
    "1. Create realistic sample datasets in each format (or find open datasets)\n",
    "2. Load each data source handling format-specific challenges\n",
    "3. Standardize column names and data types across sources\n",
    "4. Handle different customer ID formats and create unified keys\n",
    "5. Merge datasets using appropriate join strategies\n",
    "6. Identify and resolve data quality issues\n",
    "7. Create a master customer analytics dataset\n",
    "8. Export results to data warehouse format (Parquet with partitioning)\n",
    "9. Generate business intelligence reports in Excel format\n",
    "\n",
    "**Learning Focus**: Data integration, ETL processes, data quality, business analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1173f",
   "metadata": {},
   "source": [
    "## Exercise 9: Real-Time Data Processing - Financial Market Feeds\n",
    "**Data Source**: Alpha Vantage API / Yahoo Finance\n",
    "**URL**: https://www.alphavantage.co/ or Yahoo Finance API\n",
    "\n",
    "**Challenge**: \n",
    "Handle streaming financial data with:\n",
    "- Real-time price updates\n",
    "- High-frequency data points\n",
    "- Missing data during market closures\n",
    "- Different market timezones\n",
    "\n",
    "**Tasks**:\n",
    "1. Set up real-time stock price feeds for 10 major stocks\n",
    "2. Implement continuous data collection with error handling\n",
    "3. Handle rate limiting and API quotas gracefully\n",
    "4. Store streaming data incrementally (append-only approach)\n",
    "5. Implement data validation and anomaly detection\n",
    "6. Create rolling window calculations (moving averages, volatility)\n",
    "7. Handle market closure periods and timezone conversions\n",
    "8. Export hourly snapshots to compressed CSV files\n",
    "9. Maintain a real-time dashboard data feed (JSON format)\n",
    "10. Implement data archival strategy for historical data\n",
    "\n",
    "**Learning Focus**: Streaming data, real-time processing, error handling, time series management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbe5f0",
   "metadata": {},
   "source": [
    "## Exercise 10: Advanced Challenge - Social Media Analytics Pipeline\n",
    "**Data Sources**: Multiple social media APIs\n",
    "**URLs**: Twitter API v2, Reddit API, Instagram Basic Display API\n",
    "\n",
    "**Challenge**: \n",
    "Build a complete data pipeline handling:\n",
    "- Multiple API endpoints with different schemas\n",
    "- Rate limiting across multiple services\n",
    "- Text data with various encodings\n",
    "- Image metadata and binary data\n",
    "- Geolocation and timestamp normalization\n",
    "\n",
    "**Tasks**:\n",
    "1. Set up authentication for 3 different social media APIs\n",
    "2. Collect posts/tweets about a trending topic over 48 hours\n",
    "3. Handle different JSON schemas and API response formats\n",
    "4. Process text data (encoding, emojis, special characters)\n",
    "5. Extract and download linked media files\n",
    "6. Implement robust error handling and retry logic\n",
    "7. Store raw data in MongoDB or similar NoSQL database\n",
    "8. Create processed analytical datasets in multiple formats:\n",
    "   - Time series data (CSV) for trend analysis\n",
    "   - User network data (JSON) for social graph analysis\n",
    "   - Text corpus (compressed text files) for NLP\n",
    "   - Media metadata (Excel) for content analysis\n",
    "9. Implement automated data quality reporting\n",
    "10. Create a comprehensive ETL documentation report\n",
    "\n",
    "**Learning Focus**: API orchestration, NoSQL databases, text processing, pipeline automation, documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10db786",
   "metadata": {},
   "source": [
    "## Bonus Tips & Resources\n",
    "\n",
    "### Performance Considerations\n",
    "- Always profile your data loading operations\n",
    "- Use `chunksize` parameter for large files\n",
    "- Consider `usecols` to load only needed columns\n",
    "- Experiment with different `dtype` specifications\n",
    "- Use compressed formats (Parquet, HDF5) for repeated access\n",
    "\n",
    "### Data Quality Checklist\n",
    "- Check for duplicate records\n",
    "- Validate data types and ranges\n",
    "- Handle missing values appropriately\n",
    "- Document data transformations\n",
    "- Implement data validation tests\n",
    "\n",
    "### Useful Libraries Beyond Pandas\n",
    "- **fastparquet** or **pyarrow**: For Parquet files\n",
    "- **h5py**: For low-level HDF5 operations\n",
    "- **requests**: For API interactions\n",
    "- **beautifulsoup4**: For HTML parsing\n",
    "- **lxml**: For XML processing\n",
    "- **sqlalchemy**: For database connections\n",
    "- **openpyxl**: For Excel file manipulation\n",
    "\n",
    "### Additional Practice Resources\n",
    "- **Kaggle Datasets**: https://www.kaggle.com/datasets\n",
    "- **Google Dataset Search**: https://datasetsearch.research.google.com/\n",
    "- **AWS Open Data**: https://aws.amazon.com/opendata/\n",
    "- **UCI ML Repository**: https://archive.ics.uci.edu/ml/index.php\n",
    "- **Data.gov**: https://www.data.gov/\n",
    "\n",
    "Good luck with your pandas data loading journey! üêºüìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54d512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
